import numpy as np
import pickle
import random
from settings import *

class QAgent:
    def __init__(self):
        self.q_table = {} # Dicion√°rio: Chave="Estado", Valor=[Lista de Q-Values]
        self.epsilon = EPSILON_START
        self.alpha = LEARNING_RATE
        self.gamma = DISCOUNT_FACTOR

    def get_state_key(self, board):
        """
        OTIMIZA√á√ÉO: Generaliza√ß√£o de Oponentes.
        Transforma o tabuleiro para que todos os oponentes (2, 3, 4)
        pare√ßam o mesmo n√∫mero (ex: 2).
        Isso reduz drasticamente o espa√ßo de estados e acelera o aprendizado.
        """
        # Cria uma c√≥pia para n√£o alterar o jogo real
        simplified_board = board.copy()
        
        # Substitui 3 e 4 por 2. 
        # Agora o Agente v√™: 0=Vazio, 1=Eu, 2=Inimigo (Qualquer um)
        simplified_board[simplified_board == 3] = 2
        simplified_board[simplified_board == 4] = 2
        
        return str(simplified_board.flatten())

    def choose_action(self, board, valid_moves):
        """
        Algoritmo Epsilon-Greedy:
        - Com chance 'epsilon': Escolhe aleat√≥rio (Explora√ß√£o)
        - Caso contr√°rio: Escolhe a a√ß√£o com maior Q-Value (Explota√ß√£o)
        """
        state_key = self.get_state_key(board)

        # 1. Explora√ß√£o (Aleat√≥rio)
        # Se o dado cair num n√∫mero baixo, ele chuta uma posi√ß√£o v√°lida qualquer
        if random.random() < self.epsilon:
            return random.choice(valid_moves)

        # 2. Explota√ß√£o (Intelig√™ncia)
        # Se nunca viu esse estado, inicializa com zeros na mem√≥ria
        if state_key not in self.q_table:
            self.q_table[state_key] = np.zeros(BOARD_SIZE * BOARD_SIZE)

        # Pega os valores Q conhecidos para este estado
        q_values = self.q_table[state_key]

        # Filtro de Seguran√ßa:
        # Criamos uma lista tempor√°ria onde jogadas inv√°lidas t√™m valor -Inifinito
        # Isso garante que a IA nunca escolha jogar numa casa ocupada quando estiver jogando s√©rio
        masked_q_values = np.full(BOARD_SIZE * BOARD_SIZE, -np.inf)
        
        for move in valid_moves:
            masked_q_values[move] = q_values[move]
        
        # Retorna o √≠ndice da a√ß√£o com maior valor
        return np.argmax(masked_q_values)

    def learn(self, state, action, reward, next_state):
        """
        Atualiza a Q-Table usando a Equa√ß√£o de Bellman.
        Q_novo = Q_velho + alpha * [Recompensa + gamma * max(Q_futuro) - Q_velho]
        """
        state_key = self.get_state_key(state)
        next_state_key = self.get_state_key(next_state)

        # Garante que os estados existam na tabela antes de atualizar
        if state_key not in self.q_table:
            self.q_table[state_key] = np.zeros(BOARD_SIZE * BOARD_SIZE)
        if next_state_key not in self.q_table:
            self.q_table[next_state_key] = np.zeros(BOARD_SIZE * BOARD_SIZE)

        # Valores para o c√°lculo
        old_value = self.q_table[state_key][action]
        next_max = np.max(self.q_table[next_state_key]) # O melhor valor poss√≠vel do pr√≥ximo estado

        # A F√≥rmula M√°gica
        new_value = old_value + self.alpha * (reward + self.gamma * next_max - old_value)
        
        # Grava o novo conhecimento
        self.q_table[state_key][action] = new_value

    def save_model(self, filename="brain.pkl"):
        """Salva o c√©rebro treinado em um arquivo."""
        with open(filename, "wb") as f:
            pickle.dump(self.q_table, f)
        print(f"üíæ Modelo salvo em {filename} ({len(self.q_table)} estados aprendidos).")

    def load_model(self, filename="brain.pkl"):
        """Carrega um c√©rebro treinado."""
        try:
            with open(filename, "rb") as f:
                self.q_table = pickle.load(f)
            print(f"üìÇ Modelo carregado! Conhece {len(self.q_table)} situa√ß√µes.")
        except FileNotFoundError:
            print("‚ö†Ô∏è Arquivo n√£o encontrado. Iniciando agente do zero.")